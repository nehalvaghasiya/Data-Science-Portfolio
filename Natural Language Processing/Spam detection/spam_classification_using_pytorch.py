# -*- coding: utf-8 -*-
"""Spam Classification using Pytorch.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13z1Hdo9Jgkknhu-pf2l84tsevTpkHwb0

#Import the libraries
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.autograd import Variable
import torchvision
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import re
import os

import nltk
from nltk.tokenize import word_tokenize
from nltk import PorterStemmer
from nltk import WordNetLemmatizer
from nltk.corpus import stopwords

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import train_test_split

"""#Get the data
- Required steps to follow:
  1. Connect google-collab to google drive
  2. Arrange the files according to directory tree as shown in Readme.md
  3. Add the created model to the directory and use it in deployment.

"""

messages = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/natural language processing/SMS Spam detection/SMSSpamCollection', sep='\t',
                           names=["label", "message"])
messages.head()

"""#Preprocess text data

- Remove non words, lower it, then Tokenize, Lemmatize and Vectorize and Remove Stopwords from the data
"""

remove_non_alphabets =lambda x: re.sub(r'[^a-zA-Z]',' ',x)

tokenize = lambda x: word_tokenize(x)

ps = PorterStemmer()
stem = lambda w: [ ps.stem(x) for x in w ]

lemmatizer = WordNetLemmatizer()
leammtizer = lambda x: [ lemmatizer.lemmatize(word) for word in x ]

import nltk
nltk.download('punkt')
nltk.download('wordnet')

print('Processing : [=', end='')
messages['message'] = messages['message'].apply(remove_non_alphabets)
print('=', end='')
messages['message'] = messages['message'].apply(tokenize) # [ word_tokenize(row) for row in data['email']]
print('=', end='')
messages['message'] = messages['message'].apply(stem)
print('=', end='')
messages['message'] = messages['message'].apply(leammtizer)
print('=', end='')
messages['message'] = messages['message'].apply(lambda x: ' '.join(x))
print('] : Completed', end='')

messages.head()

"""#Bag of words"""

max_words = 10000
cv = CountVectorizer(max_features=max_words, stop_words='english')
sparse_matrix = cv.fit_transform(messages['message']).toarray()

sparse_matrix.shape

import pickle

with open('vectorizer.pkl', 'wb') as fout:
    pickle.dump(cv, fout)

"""#Split training and testing data"""

messages['label'].replace({"ham": 1, "spam": 0}, inplace=True)

x_train, x_test, y_train, y_test = train_test_split(sparse_matrix, np.array(messages['label']))

"""#Logistic Regression model"""

class LogisticRegression(nn.Module):
    def __init__(self):
        super(LogisticRegression, self).__init__()
        self.linear1 = nn.Linear(6177, 100)
        self.linear2 = nn.Linear(100, 10)
        self.linear3 = nn.Linear(10, 2)
        
    def forward(self, x):
        x = F.relu(self.linear1(x))
        x = F.relu(self.linear2(x))
        x = self.linear3(x)
        return x

model = LogisticRegression()
print(model)

criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(params=model.parameters() , lr=0.01)

y_train.shape

x_train = Variable(torch.from_numpy(x_train)).float()
y_train = Variable(torch.from_numpy(y_train)).long()

epochs = 20
model.train()
loss_values = []
for epoch in range(epochs):
    optimizer.zero_grad()
    y_pred = model(x_train)
    loss = criterion(y_pred, y_train)
    loss_values.append(loss.item())
    pred = torch.max(y_pred, 1)[1].eq(y_train).sum()
    acc = pred * 100.0 / len(x_train)
    print('Epoch: {}, Loss: {}, Accuracy: {}%'.format(epoch+1, loss.item(), acc.numpy()))
    loss.backward()
    optimizer.step()

"""#visualization of loss function"""

plt.plot(loss_values)
plt.title('Loss Value vs Epochs')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend(['Loss'])
plt.show()

"""#Evaluation"""

x_test = Variable(torch.from_numpy(x_test)).float()
y_test = Variable(torch.from_numpy(y_test)).long()

model.eval()
with torch.no_grad():
    y_pred = model(x_test)
    print(y_pred)
    loss = criterion(y_pred, y_test)
    pred = torch.max(y_pred, 1)[1].eq(y_test).sum()
    print ("Accuracy : {}%".format(100*pred/len(x_test)))

torch.max(y_pred, 1)[1].eq(y_test)[-3]

y_test

"""#Save the model using torh"""

model = LogisticRegression()
torch.save(model,'model.pth')

print(model.state_dict)



