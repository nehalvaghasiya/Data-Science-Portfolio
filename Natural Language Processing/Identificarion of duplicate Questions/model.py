# -*- coding: utf-8 -*-
"""Identification of Duplicate Question using Deep Learning.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14uSwS-H_--ZFV9pFXzfmTajs59sfFWGY
"""



"""#Identification of Duplicate Question

Method:

- converting Questions into vectors using Word2vec
- using Siamese network to detect if the pair is duplicate

#Import Libraries

import sys
import os 
import pandas as pd
import numpy as np
from tqdm import tqdm
"""

import pandas as pd
import numpy as np

from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences

"""
#Get the data
- Required steps to follow:
  1. Connect google-collab to google drive
  2. Arrange the files according to directory tree as shown in Readme.md
  3. Add the created model to the directory and use it in deployment.

"""

data=pd.read_csv('/content/drive/MyDrive/Colab Notebooks/natural language processing/Identificarion of duplicate Questions/train.csv')
data.head()

from sklearn.model_selection import train_test_split
X = data[['question1','question2']]
y = data['is_duplicate']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)

X_train['question1'] = X_train['question1'].apply(lambda x: (str(x)))
X_train['question2'] = X_train['question2'].apply(lambda x: (str(x)))


X_test['question1'] = X_test['question1'].apply(lambda x: (str(x)))
X_test['question2'] = X_test['question2'].apply(lambda x: (str(x)))

question1 = list(X_train['question1'])
question2 = list(X_train['question2'])

test1=list(X_test['question1'])
test2=list(X_test['question2'])

"""- Tokenize the corpus, then transform sentence into sequences of integer corresponding to tokenizer word index"""

tokenizer = Tokenizer(num_words=200000)
tokenizer.fit_on_texts(question1+question2+test1+test2)

import pickle

# saving tokenizer
with open('tokenizer.pickle', 'wb') as handle:
    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)

question1_word_sequences = tokenizer.texts_to_sequences(question1)
question2_word_sequences = tokenizer.texts_to_sequences(question2)
word_index = tokenizer.word_index #unique words in corpus (training and test sets)

print("Words in index: %d" % len(word_index))

test1_word_sequences = tokenizer.texts_to_sequences(test1)
test2_word_sequences = tokenizer.texts_to_sequences(test2)

(test1_word_sequences[0])

q1_data = pad_sequences(question1_word_sequences, maxlen=25)
q2_data = pad_sequences(question2_word_sequences, maxlen=25)
test1_data=pad_sequences(test1_word_sequences, maxlen=25)
test2_data=pad_sequences(test2_word_sequences, maxlen=25)

labels = np.array(y, dtype=int)
print('Shape of question1 data tensor:', q1_data.shape)
print('Shape of question2 data tensor:', q2_data.shape)
print('Shape of label tensor:', labels.shape)

test1_data[0]

"""#Word embedding dictionary"""

embeddings_index = {}
f = open('/content/drive/MyDrive/Colab Notebooks/natural language processing/Identificarion of duplicate Questions/glove.6B.100d.txt')
for line in f:
    values = line.split()
    word = values[0]
    coefs = np.asarray(values[1:], dtype='float32')
    embeddings_index[word] = coefs
f.close()

print('Found %s word vectors.' % len(embeddings_index))

embedding_matrix = np.zeros((len(word_index) + 1, 100))
for word, i in word_index.items():
    embedding_vector = embeddings_index.get(word)
    if embedding_vector is not None:
        # words not found in embedding index will be all-zeros.
        embedding_matrix[i] = embedding_vector

embedding_matrix.shape



"""#Build the model"""

from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation,GlobalAveragePooling1D,Lambda,Bidirectional
from keras.models import Model
from keras.layers.normalization import BatchNormalization
from keras.callbacks import EarlyStopping
from keras.optimizers import Adam, RMSprop
from keras import backend as K

def vec_distance(vects):
    x, y = vects
    return K.sum(K.square(x - y), axis=1, keepdims=True)

def vec_output_shape(shapes):
    shape1, shape2 = shapes
    return (shape1[0], 1)

from keras.layers.embeddings import Embedding

nb_words=embedding_matrix.shape[0]
max_sentence_len=25
embedding_layer = Embedding(nb_words,embedding_matrix.shape[1],
        weights=[embedding_matrix],
        input_length=max_sentence_len,trainable=False)

lstm_layer =LSTM(128)

sequence_1_input = Input(shape=(max_sentence_len,), dtype='int32')
embedded_sequences_1 = embedding_layer(sequence_1_input)
x1 = lstm_layer(embedded_sequences_1)

sequence_2_input = Input(shape=(max_sentence_len,), dtype='int32')
embedded_sequences_2 = embedding_layer(sequence_2_input)
y1 = lstm_layer(embedded_sequences_2)

distance=Lambda(vec_distance, output_shape=vec_output_shape)([x1, y1])
dense1=Dense(16, activation='sigmoid')(distance)
dense1 = Dropout(0.3)(dense1)

bn2 = BatchNormalization()(dense1)
prediction=Dense(1, activation='sigmoid')(bn2)

model = Model([sequence_1_input, sequence_2_input], prediction)

model.summary()

model.compile(loss='binary_crossentropy',
        optimizer='adam',
        metrics=['acc'])

np.stack((test1_data, test2_data), axis=1)[:,1]

early_stopping =EarlyStopping(monitor='val_loss', patience=3)

X = np.stack((q1_data, q2_data), axis=1)
target = labels


Q1_train = np.stack((q1_data, q2_data), axis=1)[:,0]
Q2_train =np.stack((q1_data, q2_data), axis=1)[:,1]
Q1_val =  np.stack((test1_data, test2_data), axis=1)[:,0]
Q2_val =  np.stack((test1_data, test2_data), axis=1)[:,1]

training =model.fit([Q1_train, Q2_train], y_train, validation_data=([Q1_val, Q2_val], y_test), verbose=1, epochs=5, batch_size=256, shuffle=True,class_weight=None, callbacks=[early_stopping])

"""#Save the model"""

from keras.models import model_from_json

model_json = model.to_json()
with open("brnn_model_distance_128_d16_d05.json", "w") as json_file:
    json_file.write(model_json)
    
# serialize weights to HDF5
model.save_weights("brnn_model_distance_128_d16_d05.h5")
print("Saved model to disk")

# load json and create model
from keras.models import model_from_json
json_file = open('/content/drive/MyDrive/Colab Notebooks/natural language processing/Identificarion of duplicate Questions/brnn_model_distance_128_d16_d05.json', 'r')
loaded_model_json = json_file.read()
json_file.close()

model = model_from_json(loaded_model_json)

# load weights into new model
model.load_weights("/content/drive/MyDrive/Colab Notebooks/natural language processing/Identificarion of duplicate Questions/brnn_model_distance_128_d16_d05.h5")
print("Loaded model from disk")

"""#Prediction"""

pred=model.predict([test1_data, test2_data],verbose=1)

Q1 = 'What is the meaning of healthy routine'
Q2 = 'What should we do to follow healthy routine'
# prediction = model.predict(X = np.stack((Q1,Q2), axis=1), verbose=1)

import pandas as pd
import numpy as np

from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences

def tokenize(Q1, Q2):
  que1 = tokenizer.texts_to_sequences(list(Q1.split()))
  que2 = tokenizer.texts_to_sequences(list(Q2.split()))
  qu1 =(np.array(que1)).flatten()
  qu2 =(np.array(que1)).flatten()
  q1 = np.pad(qu1, (25-len(qu1),0), 'constant')
  q2 = np.pad(qu2, (25-len(qu2),0), 'constant')
  return q1, q2

que1 = tokenizer.texts_to_sequences(list(Q1.split()))
que2 = tokenizer.texts_to_sequences(list(Q2.split()))

qu1 =(np.array(que1)).flatten()
qu2 =(np.array(que1)).flatten()

q1 = np.pad(qu1, (25-len(qu1),0), 'constant')
q2 = np.pad(qu2, (25-len(qu2),0), 'constant')

q1

predi=model.predict([q1, q2],verbose=1)

p = np.mean(predi)

(np.where(p > 0.5, 1, 0)).flatten()

y_pred =(np.where(pred > 0.5, 1, 0)).flatten()

y_pred

y_true = np.array(y_test)

from sklearn.metrics import accuracy_score
accuracy_score(y_true, y_pred)

from sklearn.metrics import confusion_matrix,classification_report

confusion_matrix(y_true, y_pred)

target_names = ['class 0', 'class 1']
print(classification_report(y_true, y_pred, target_names=target_names))

A = np.array([1,2,3,4,5])

len(np.pad(A, (25-len(A),0), 'constant'))

